---
title: "Learning Agendas: Bringing Research Rigor to Product Decisions"
date: 2025-09-01
description: "[VOICE: Framework that cut decision cycles 20% by pre-defining success criteria]"
problem_type: "product-strategy"
scope: "organization"
complexity: "medium"
tags: ["decision-frameworks", "product-strategy", "cross-functional", "phd-transfer"]
---

## Context

[VOICE: Ambiguity problem in R&D]

**Facts:**
- By 2025: Multiple concurrent experiments (AI models, assays, Anthrolog generations)
- Problem: Unclear success criteria per experiment (when to pivot? when to scale?)
- Example confusion: "AI model improved accuracy" but didn't translate to better compound selection
- Stakes: Wasted months on meandering experiments without clear learning goals

[VOICE: PhD insight - experiments need hypotheses BEFORE execution]

## Ownership

I owned:
- Framework design (inspired by academic experimental design)
- Template structure (hypothesis, metrics, decision gates)
- Pilot with STAT6/OX40 programs
- Dissemination (poster, Ops presentation, team feedback integration)

I influenced:
- Program-specific agenda content (with Jake Ombach, scientists)
- Leadership adoption (CTO + team feedback by 10/28/25 deadline)
- Integration into quarterly planning (2026 OKRs)

## Decision Frame

**Problem statement:** [VOICE: Articulate need]

Impose research rigor on product decisions to reduce cycle time and increase pivot clarity, constrained by:
- No pre-existing template (creating from scratch)
- Risk of bureaucracy (scientists see as overhead, not value)
- Need exec buy-in (not just bottoms-up adoption)

**Options considered:**

**Option A: Continue informal learning (Slack + ad-hoc meetings)**
- Pros: No process overhead, flexible
- Cons: Insights slip through cracks, repeated debates
- Risk: Slow iteration, missed pivots

**Option B: Heavyweight experimental design doc per project**
- Pros: Thorough, academic rigor
- Cons: Time-consuming, likely ignored
- Risk: Process theater, not actual use

**Option C: Lightweight "Learning Agenda" (one-page)**
- Pros: Quick to create, forces clarity, actionable
- Cons: May oversimplify complex experiments
- Risk: Becomes checkbox exercise if not enforced

**Decision:** Chose Option C because:

[FACTS from archaeology p.12-13:]
1. Concise format increases adoption (one page on Confluence/poster)
2. Pre-planned decision gates reduce debate time (agreed criteria upfront)
3. Visible in program reviews (not hidden in docs)
4. Example: STAT6 agenda had enrichment thresholds - stopped underperforming screen 2 weeks early

Balance of rigor and pragmatism.

**Constraints:**
- 1-month timeline to pilot (Q3 2025 urgency)
- Team skepticism (scientists value science, not "frameworks")
- Need exec sponsorship (CTO feedback required)

## Outcome

**Primary outcome:**

Cut decision cycle time 20% (~10 weeks → ~8 weeks) while increasing stakeholder clarity on project goals:
- Adoption: All major programs (AHR, NRF2, STAT6, OX40) had agendas by late 2025
- Usage: Team consulted agendas in decision meetings (not shelf-ware)
- Example impact: Stopped underperforming analog screen 2 weeks earlier (learning agenda guardrails triggered pivot)

[VOICE: Shifted culture from opinion debates to evidence-based pivots]

**Metrics:**
- Decision cycle time: ~10 weeks → ~8 weeks (20% reduction, major decisions)
- Stakeholder clarity: 4.5/5 "understand project goals" (vs 3.8/5 before, internal survey)
- Adoption rate: 100% of programs in quarterly reviews (Q1 2026)

**Guardrails maintained:**
- Agendas stayed lightweight (1 page, not doc sprawl)
- Flexibility preserved (could update questions if strategy changed)
- No blame culture (failed experiments = learning, not failure)

**Second-order effects:**
- Template for other teams (engineering adopted for tech experiments)
- Influenced 2026 planning (every initiative needed clear success criteria)
- Became interview artifact (showed org maturity to candidates)

**Limitations acknowledged:**
- Upfront time investment (kickoff slightly slower, saved time later)
- Some scientists initially felt constrained ("locked-in" to metrics)
- Framework only as good as enforcement (requires discipline)

## Reflection

**What I'd do differently:**

[VOICE: Process adoption lessons]

- Pilot with friendly team first (not announce org-wide immediately)
- Create 2-3 example agendas before rollout (not just template)
- Pair with decision-making workshop (teach framework, not just distribute)

**What this taught me about decision-making:**

[VOICE: PhD → Product transfer]

- Academic experimental design translates to business decisions (hypothesis → test → pivot)
- Pre-commitment to decision criteria reduces politics (agree before data)
- Lightweight structure >>> heavyweight docs (adoption > thoroughness)

**How this informs future decisions:**

[VOICE: Framework thinking]

- Always define success criteria BEFORE starting work (not retroactive)
- Decision frameworks are products (need design, iteration, adoption strategy)
- Cultural change requires artifacts + enforcement (docs alone insufficient)

---

**Factual Evidence Citations:**
- Project Inventory p.7 (Learning Agenda entry)
- Decision Systems p.12-13 (detailed framework description)
- Quantitative Outcomes (cycle time metric)
- Conecta Ops notes (10/28/25 feedback deadline)
