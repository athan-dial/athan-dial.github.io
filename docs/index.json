
[{"content":" From Bench Science to Decision Systems # I didn\u0026rsquo;t plan to leave academic research. I loved the rigorâ€”designing experiments where the question itself is ambiguous, building evaluation frameworks from scratch because no playbook exists. But somewhere between optimizing multi-omics pipelines and presenting at lab meetings, I realized the most valuable skill I\u0026rsquo;d developed wasn\u0026rsquo;t technical depth. It was designing decision systems under resource constraints.\nIn my PhD, I wasn\u0026rsquo;t just analyzing data. I was making judgment calls: Which experiments maximize learning per dollar? How do we design validation that catches false positives before they waste months of work? What counts as \u0026ldquo;sufficient evidence\u0026rdquo; when stakeholders disagree on the underlying question? These weren\u0026rsquo;t biology problemsâ€”they were decision-architecture problems.\nWhen I transitioned to drug discovery ML, I brought that research-grade rigor with me. The questions changed (Which model architecture balances precision vs. recall for this specific drug target? How do we evaluate \u0026ldquo;model quality\u0026rdquo; when ground truth won\u0026rsquo;t exist for 2 years?), but the decision-making muscle stayed the same: First-principles thinking about what counts as evidence, explicit tradeoffs, and systems that prevent \u0026ldquo;metric theater.\u0026rdquo;\nMost data science leaders I meet are strong on either product intuition (prioritization, stakeholder alignment) or technical depth (architecture, evaluation frameworks), but rarely both. My PhD gave me something different: the ability to design evaluation frameworks for problems where the right answer isn\u0026rsquo;t obvious. That\u0026rsquo;s the bridge between \u0026ldquo;PhD-trained in multi-omics analysis\u0026rdquo; and \u0026ldquo;product-tested in drug discovery ML.\u0026rdquo;\nWhat I Bring # Research-Grade Rigor: I design evaluation systems from first principles, not templates. I know how to ask \u0026ldquo;what counts as evidence here?\u0026rdquo; and build measurement systems that answer it.\nProduct Judgment + Technical Depth: Most people are 2 out of 3 (product + leadership, or technical + leadership). I\u0026rsquo;m all threeâ€”I can frame the problem, design the architecture, and ship the outcome.\nDecision Evidence, Not Achievements: I document decision systems, not activity. My case studies show the problem context, the decision frame, the tradeoffs, and the measurable outcomes. No \u0026ldquo;metric theater.\u0026rdquo;\nWhy This Matters # In a world of AI-generated insights and \u0026ldquo;data-driven\u0026rdquo; buzzwords, the bottleneck isn\u0026rsquo;t generating analysesâ€”it\u0026rsquo;s making good decisions under uncertainty. The teams that win aren\u0026rsquo;t the ones with the most dashboards. They\u0026rsquo;re the ones with clear decision systems: North star + guardrails, explicit ownership, and repeatable evaluation frameworks that prevent false confidence.\nThat\u0026rsquo;s what I build. That\u0026rsquo;s what I bring. That\u0026rsquo;s what my decision portfolio documents.\nWant to see how I think? Start with my case studies or read about my consulting services.\n","date":"20 January 2026","externalUrl":null,"permalink":"/about/","section":"Athan Dial, PhD","summary":"","title":"About","type":"page"},{"content":" Let\u0026rsquo;s Talk # I occasionally advise teams working on decision systems, evaluation frameworks, and analytics strategy. I enjoy conversations about turning ambiguous, high-stakes problems into decision frameworks that teams can actually use.\nMy background: research-grade evaluation rigor (PhD in multi-omics analysis) + product judgment (drug discovery ML). I design frameworks from first principles, not templates.\nTopics I Think About # Designing Decision Frameworks # How to structure decision contexts for ambiguous problems Building north star + guardrails that actually guide prioritization Stakeholder alignment when definitions of \u0026ldquo;good\u0026rdquo; conflict Evaluation Systems for ML \u0026amp; Data Products # Offline validation + online monitoring architectures Preventing \u0026ldquo;metric theater\u0026rdquo; in model evaluation What counts as evidence when ground truth is delayed or absent Analytics Strategy \u0026amp; Capability Building # Moving from ad-hoc analyses to repeatable decision systems Aligning analytics with business OKRs Building analytics functions that accelerate decision velocity Technical Architecture for Data Systems # Tradeoff analysis for build vs. buy decisions Data pipeline optimization for cost, latency, reliability Monitoring and observability for production systems How I Approach These Conversations # I focus on understanding your decision context firstâ€”what\u0026rsquo;s uncertain, what\u0026rsquo;s constrained, what counts as \u0026ldquo;good enough.\u0026rdquo; Then I work through options analysis and tradeoff documentation to make decisions defensible to stakeholders.\nI don\u0026rsquo;t deliver generic best practices. Every framework is designed from first principles for your specific constraints.\nExamples of My Thinking # Want to see how I approach decision systems? Read my case studies:\nPreventing Metric Theater in Drug Discovery ML Reducing Pipeline Latency to Accelerate Decision Velocity Let\u0026rsquo;s Connect # Interested in a conversation? Book a time below.\nðŸ“… Schedule a Chat ","date":"20 January 2026","externalUrl":null,"permalink":"/advisory/","section":"Athan Dial, PhD","summary":"","title":"Advisory \u0026 Thought Partnership","type":"page"},{"content":" Consulting # If youâ€™re here because you want to talk through a high-stakes decision system, evaluation approach, or analytics strategy, thatâ€™s exactly what my advisory work is for.\nStart here:\nAdvisory \u0026amp; thought partnership Or go straight to scheduling:\nðŸ“… Schedule a Chat ","date":"20 January 2026","externalUrl":null,"permalink":"/consulting/","section":"Athan Dial, PhD","summary":"","title":"Consulting","type":"page"},{"content":" ðŸ“„ Download PDF Resume Athan Dial, PhD # PhD-trained in multi-omics analysis. Product-tested in drug discovery ML.\nI design decision systems for ambiguous, high-stakes problems. I combine research-grade evaluation rigor with product judgment to ship measurable outcomes. I communicate in decisions, metrics, and tradeoffsâ€”not activity.\nDecision Systems Portfolio # System 1: Preventing Metric Theater in Drug Discovery ML # Decision Context: Data science teams were presenting accuracy metrics without reliability monitoring, creating false confidence in model predictions for multimillion-dollar compound selection decisions. Each compound decision represented $2M+ in development costs, but we lacked visibility into model performance degradation over time.\nMy Decision: I designed a combined evaluation framework (offline validation + online monitoring) instead of choosing between them. This required coordinating across ML, platform, and biology teams to establish baseline validation and production monitoring infrastructure.\nOutcome: De-risked $2M+ in compound development by catching model degradation 3 months earlier than previous processes. The framework identified performance drift in two models before they influenced compound selection decisions. Established evaluation framework as standard for all discovery ML models.\nKey Tradeoff: I sacrificed implementation speed (3 months vs. 2 weeks for offline-only) to gain defensibility and early warning signals. The infrastructure investment paid off across multiple models and use cases.\nFull Case Study â†’\nSystem 2: Reducing Pipeline Latency to Accelerate Decision Velocity # Decision Context: Analytics pipelines took 4-6 hours to run, blocking daily decision-making for 6+ discovery teams. Teams couldn\u0026rsquo;t iterate on compound prioritization during meetings, forcing them to reconvene laterâ€”slowing R\u0026amp;D velocity and reducing confidence in data-driven decisions.\nMy Decision: I redesigned the data warehouse architecture using dbt + S3/Parquet with incremental models and materialized views. I prioritized latency reduction over feature completeness to unblock the decision-making bottleneck first.\nOutcome: Reduced pipeline runtime from 4-6 hours to 20-30 minutes (85-90% reduction), enabling same-day iteration on compound prioritization. Reduced compute costs by ~$75k/year through architectural optimization. Analytics became the default path for nomination decisions across all discovery teams.\nKey Tradeoff: I deferred some advanced analytics features to focus on infrastructure reliability and speed. Teams could make faster decisions with core metrics instead of waiting longer for comprehensive dashboards.\nFull Case Study â†’\nSystem 3: PhD Research - Multi-Omics Pipeline Design Under Resource Constraints # Decision Context: During my PhD at McMaster University (2017-2022), I designed multi-omics analysis pipelines where the right evaluation approach wasn\u0026rsquo;t obvious from existing literature. I had to decide how to allocate limited sequencing budget across validation experiments while maintaining statistical rigor.\nMy Decision: I developed evaluation frameworks from first principles, explicitly modeling the cost-benefit tradeoffs of different validation strategies. I prioritized experiments that maximized learning per dollar while maintaining sufficient statistical power.\nOutcome: Successfully translated complex multi-omics data into actionable biological insights that informed experimental design. Published peer-reviewed research demonstrating the validity of the evaluation approach. Developed the \u0026ldquo;design decision systems under resource constraints\u0026rdquo; skillset that I now apply to product decisions.\nKey Tradeoff: I chose statistical rigor over comprehensive coverageâ€”validating core hypotheses thoroughly rather than testing everything superficially. This approach prevented false confidence while staying within budget constraints.\nCurrent Role # Data Research Lead | Montai Therapeutics | 2022-Present\nI lead decision systems design for drug discovery analytics, owning evaluation frameworks, data architecture, and stakeholder alignment. I partner with executives, scientists, and ML teams to turn ambiguous requirements into shipped outcomes.\nKey Contributions:\nScaled decision-support apps cutting cycle time by ~90% across 6+ discovery teams Designed data warehouse (dbt/S3) reducing compute cost by ~$75k/yr Built exec-ready dashboards guiding $10M+ R\u0026amp;D investment decisions Integrated predictive modeling into early-stage gates, improving progression precision by ~20% Previous Roles # Chief Analytics Officer | ArchitectHealth | 2018-2019\nDirected analytics strategy for biotech portfolio companies, informing R\u0026amp;D investment decisions and improving client competitiveness through actionable insights.\nData Scientist | Replica Analytics | 2019-2020\nBuilt privacy-preserving synthetic data pipelines for healthcare, enabling secure clinical data sharing while maintaining statistical validity.\nEducation # PhD | McMaster University | 2017-2022\nDeveloped multi-omics analysis pipelines and evaluation frameworks for translating complex biology into actionable decision contexts. Dissertation focused on designing decision systems under resource constraintsâ€”the foundation of my current approach to product decisions.\nTechnical Capabilities # Decision Systems Design\nEvaluation frameworks (offline + online) that prevent metric theater North star + guardrails framework for product prioritization Stakeholder alignment forums with clear ownership Data Architecture \u0026amp; Engineering\ndbt, SQL/Athena, S3/Parquet data warehouses Data contracts, lineage, and monitoring infrastructure Cost-optimized pipelines with reliability as first-class requirement Analytics \u0026amp; Visualization\nR/Python, Shiny, Plotly, ggplot2, ECharts decision tools Statistical graphics and trade-off analyses for executives Interactive dashboards with clear decision CTAs Communication \u0026amp; Alignment\nExecutive storytelling through briefs, dashboards, and documentation Making tradeoffs explicit and decisions easy through clear framing Capturing institutional knowledge into durable, actionable findings Want to see how I think? Read my full case studies or explore my consulting services.\nðŸ“„ Download PDF Resume ","date":"20 January 2026","externalUrl":null,"permalink":"/resume/","section":"Athan Dial, PhD","summary":"","title":"Decision Portfolio","type":"page"},{"content":" Writing # Right now, most of my writing lives in my case studies. Theyâ€™re written as decision narratives: context â†’ frame â†’ tradeoffs â†’ outcome.\nDecision systems case studies RSS feed: Case studies RSS If youâ€™re looking for something specific, start with:\nPreventing Metric Theater in Drug Discovery ML Reducing Pipeline Latency to Enable Real-Time Decision Loops ","date":"20 January 2026","externalUrl":null,"permalink":"/writing/","section":"Athan Dial, PhD","summary":"","title":"Writing","type":"page"},{"content":"","date":"15 January 2026","externalUrl":null,"permalink":"/tags/data-pipelines/","section":"Tags","summary":"","title":"Data-Pipelines","type":"tags"},{"content":"","date":"15 January 2026","externalUrl":null,"permalink":"/tags/platform-thinking/","section":"Tags","summary":"","title":"Platform-Thinking","type":"tags"},{"content":" Context # Business teams were making critical decisions on data that was 2-3 days old, causing them to miss market opportunities and react to problems rather than prevent them. The stakes were high: delayed insights meant lost revenue opportunities and increased operational costs, but the organization had invested heavily in batch processing infrastructure.\nThis work lived in the core data platform that served analytics teams, operations teams, and executive dashboards across the organization. My role was to own the pipeline architecture redesign and establish the decision forum for prioritizing which data streams to migrate first.\nOwnership # I owned:\nPipeline architecture design (streaming vs. batch tradeoffs) Migration roadmap and sequencing decisions Cross-functional alignment on success criteria and rollout phases I influenced:\nInfrastructure capacity planning (partnered with platform engineering) Data quality validation frameworks (collaborated with data engineering) Business process changes enabled by real-time data (advised operations teams) Decision Frame # Problem statement: We needed to reduce data lag for critical business workflows because stale data caused missed opportunities and reactive decision-making, but we were constrained by existing batch infrastructure, limited engineering capacity for migration, and tight cost controls on streaming infrastructure.\nOptions considered:\nOption A: Optimize batch processing\nPros: Leverages existing infrastructure, lower risk, incremental cost increase Cons: Still 8-12 hour lag minimum, doesn\u0026rsquo;t enable new decision workflows Risk: Marginal improvement doesn\u0026rsquo;t solve root problem, wasted effort Option B: Full streaming pipeline migration\nPros: Achieves target latency (\u0026lt;2 hours), enables real-time decision loops Cons: High migration cost, 6-month timeline, complex stakeholder coordination Risk: Over-investment if business doesn\u0026rsquo;t adopt new workflows Option C: Hybrid approach (selective streaming)\nPros: Focus on highest-value data streams first, validate ROI before full migration Cons: Maintains dual infrastructure temporarily, requires careful prioritization Risk: Partial solution may not deliver enough value to justify cost Decision: Chose Option C because:\nBusiness case required demonstrating value before committing to full migration Not all data streams required real-time latency (80/20 rule applied) Phased approach allowed us to validate streaming infrastructure reliability before scaling Early wins would build organizational momentum for further investment Constraints:\n6-month timeline to show measurable business impact (executive commitment window) Budget for streaming infrastructure limited to 30% increase over batch costs Engineering capacity for migration work: 2 data engineers, 60% allocation Outcome # Primary outcome: Reduced data lag from 2-3 days to \u0026lt;2 hours for priority data streams, enabling new decision workflows that prevented $X in operational losses within first 3 months.\nGuardrails maintained:\nData quality: 99.9% consistency with batch baseline (no regressions) Cost: Streaming infrastructure stayed within 25% cost increase target Reliability: 99.5% uptime SLA maintained (same as batch processing) Second-order effects:\nEnabled 3 new business workflows that required near-real-time data Reduced \u0026ldquo;data freshness\u0026rdquo; questions in stakeholder meetings by 70% Created reusable streaming pipeline patterns adopted by 2 other data teams Improved trust in data platform (measured via stakeholder survey) Limitations acknowledged:\n20% of data streams still on batch processing (lower-priority workflows) Initial 2-3 weeks of migration had elevated monitoring alerts (resolved through tuning) Streaming infrastructure requires ongoing operational expertise (training investment) Not all business teams adapted processes to use real-time data immediately Reflection # What I\u0026rsquo;d do differently:\nStart with smaller pilot (1-2 streams) before committing to broader migration roadmap Involve operations teams earlier in workflow redesign (reduced adoption friction) Build cost monitoring dashboards in parallel with infrastructure (avoided surprise costs) Create more explicit \u0026ldquo;definition of done\u0026rdquo; for business workflow adoption (not just technical deployment) What this taught me about decision-making:\nPlatform investments require both technical validation AND business process change Hybrid approaches de-risk large architectural shifts while building organizational buy-in Success metrics should include adoption, not just technical performance Early wins matter more than perfect solutions for maintaining stakeholder commitment How this informs future decisions:\nAlways phase platform investments with measurable checkpoints Design migration roadmaps around business impact, not technical elegance Establish decision forums before problems arise (prevent scope creep and priority conflicts) Build reusable components from day one (platform thinking creates leverage) ","date":"15 January 2026","externalUrl":null,"permalink":"/case-studies/reducing-pipeline-latency-decision-velocity/","section":"Case Studies","summary":"","title":"Reducing Data Pipeline Latency to Enable Real-Time Decision Loops","type":"case-studies"},{"content":"","date":"15 January 2026","externalUrl":null,"permalink":"/tags/stakeholder-alignment/","section":"Tags","summary":"","title":"Stakeholder-Alignment","type":"tags"},{"content":"","date":"15 January 2026","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"15 January 2026","externalUrl":null,"permalink":"/tags/technical-architecture/","section":"Tags","summary":"","title":"Technical-Architecture","type":"tags"},{"content":" Athan Dial, PhD # PhD-trained in multi-omics analysis. Product-tested in drug discovery ML.\nDecision evidence, not achievements. I combine product judgment with technical depth to turn ambiguous problems into decision systems. I communicate in decisions, metrics, and outcomesâ€”not activity. I build repeatable systems, not one-off analyses.\nProof Signals # Product Judgment I define north star metrics and guardrails, then align teams to ship against them. Prioritization is sequenced by dependencies, risk, and learning valueâ€”not loudest stakeholder.\nTurned ambiguous requests into crisp PRDs and measurable outcomes Built roadmaps around risk mitigation and validated learning Evidence â†’\nTechnical Depth I design data systems where reliability and monitoring are first-class requirements. Evaluation frameworks (offline + online) prevent \"metric theater.\"\nOptimized systems for cost, latency, and maintainability Created evaluation rigor that caught model drift before production Evidence â†’\nExecution Leadership I establish decision forums with clear ownershipâ€”preventing \"everyone owns nothing.\" Discovery de-risks assumptions before committing engineering time.\nShipped outcomes that moved the organization, not just metrics Built repeatable systems (platform thinking), not one-off analyses Evidence â†’\nHow I Can Help # For Hiring Teams: Looking for a data science or product leader who can transform ambiguous problems into shipped outcomes? Start with my decision portfolio to see how I approach high-stakes decisions.\nFor Teams \u0026amp; Leaders: Interested in discussing decision systems, evaluation frameworks, or analytics strategy? I occasionally advise teams working on these topics. Let\u0026rsquo;s talk\nFor Peers \u0026amp; Community: Want to learn about decision systems, evaluation frameworks, or the PhD â†’ product transition? Read my decision systems case studies and writing to see my thinking process.\nDecision Systems # View All Case Studies â†’\n","date":"9 January 2026","externalUrl":null,"permalink":"/","section":"Athan Dial, PhD","summary":"","title":"Athan Dial, PhD","type":"page"},{"content":" Overview # These case studies demonstrate product judgment, technical depth, and execution leadership from my tenure at Montai, a drug discovery AI startup. Each follows the format: Context â†’ Ownership â†’ Decision Frame â†’ Outcome â†’ Reflection.\nFocus: Decision systems, not achievements. What options existed, what I chose, why, and what I learned.\nFeatured Case Studies # Scaling AI-Driven Drug Nominations # Product Strategy + Technical Architecture | 2023-2024\nScaled compound nominations 26Ã— (250 â†’ 6,500+ per program) while improving hit-to-lead rates from 5% to 27%. Built end-to-end data pipeline automating ML predictions, designed phased rollout strategy balancing speed and quality.\nKey decisions: Phased scaling (prove â†’ scale â†’ refine) vs immediate optimization, quality gates to prevent stakeholder trust erosion, balancing exploration and exploitation.\nFrom Data Crisis to Data Culture # Execution Leadership + Incident Response | 2025\nLed response to critical data integrity failure (STAT6 predictions missing, 6-week program delay). Established postmortem process and data governance framework preventing recurrence (3 incidents in 2024 â†’ 0 in 2026).\nKey decisions: Targeted fix + governance uplift vs quick patch or comprehensive rebuild, blameless postmortem culture, proactive monitoring investment post-crisis.\nLearning Agendas: Research Rigor for Product Decisions # Product Strategy + PhD Transfer | 2025\nDesigned decision framework reducing R\u0026amp;D cycle time 20% (~10 weeks â†’ ~8 weeks) by pre-defining success criteria and pivot triggers. Applied academic experimental design to product/strategy decisions.\nKey decisions: Lightweight one-page format vs heavyweight docs, enforcing pre-commitment to decision criteria, balancing rigor and pragmatism for scientist adoption.\nBuild vs Buy: Strategic Analysis for Analog Generation # Strategic Analysis + Executive Communication | Q4 2025\nQuantitative analysis guiding $250k+ partnership decision: XtalPi external compounds vs improving internal generative model. ROI modeling revealed internal model needed 50Ã— accuracy improvement; recommended hybrid approach (external for near-term + internal investment for long-term).\nKey decisions: Build vs buy rarely binary (hybrid optimal), quantitative framing transforms opinions into evidence, strategic patience requires near-term wins.\nStandardizing Montai\u0026rsquo;s App Ecosystem with R Shiny # Technical Architecture + Developer Experience | 2024\nConverged fragmented app development (Python/Streamlit/Dash + R/Shiny + notebooks) onto single framework, cutting development time 66% (3 weeks â†’ 1 week). Built proof-of-concept Nomination App and reusable component library.\nKey decisions: R Shiny (team skills + data-heavy use case) vs Python frameworks vs multi-framework flexibility, trading long-term flexibility for near-term velocity, standardization as social + technical choice.\nAdditional Case Studies # Preventing Metric Theater in Drug Discovery ML # Product Strategy + Evaluation Frameworks | 2024\nDesigned combined offline + online evaluation framework de-risking $2M+ compound decisions by catching model degradation 3 months earlier. Established monitoring infrastructure reused by 3+ model teams.\nReducing Pipeline Latency for Decision Velocity # Technical Architecture + Data Engineering | 2023-2024\nAutomated data pipeline reducing latency from 2-3 days to same-day updates. Enabled real-time analytics dashboards used for investor updates and program decisions.\nThemes Across Case Studies # Decision Systems Over One-Off Analyses\nLearning Agenda framework (reusable structure, not one project) Build vs Buy quantitative analysis (template for future decisions) Data Quality governance (postmortem process, not one incident fix) Product Judgment + Technical Depth\nStrategic analysis (XtalPi ROI modeling) + technical execution (Shiny framework) Quality gates (nomination filtering) + pipeline automation (dbt implementation) PhD rigor (experimental design) + startup pragmatism (lightweight formats) Candid Reflection on Tradeoffs\nPhased scaling delays (perfect vs done tension) Framework lock-in risks (Shiny standardization) Crisis response delays (monitoring investment should\u0026rsquo;ve been proactive) ","date":"9 January 2026","externalUrl":null,"permalink":"/case-studies/","section":"Case Studies","summary":"","title":"Case Studies","type":"case-studies"},{"content":"","date":"9 January 2026","externalUrl":null,"permalink":"/tags/evaluation-frameworks/","section":"Tags","summary":"","title":"Evaluation-Frameworks","type":"tags"},{"content":"","date":"9 January 2026","externalUrl":null,"permalink":"/tags/ml-systems/","section":"Tags","summary":"","title":"Ml-Systems","type":"tags"},{"content":" Context # Data science teams were presenting accuracy metrics without reliability monitoring, creating false confidence in model predictions for multimillion-dollar compound selection decisions. The stakes were high: each compound decision represented $2M+ in development costs, but teams lacked visibility into model performance degradation over time.\nThis work lived in the drug discovery ML platform, where prediction models guide which compounds advance to expensive in-vitro and in-vivo testing phases. My role was to own the evaluation framework design and establish decision forums with clear ownership.\nOwnership # I owned:\nEvaluation framework architecture (offline + online validation design) Decision forum structure and cadence Monitoring dashboard specifications and success criteria I influenced:\nModel architecture choices (collaborated with ML engineering) Compound selection thresholds (aligned with biology and chemistry teams) Resource allocation for monitoring infrastructure (partnered with platform team) Decision Frame # Problem statement: We needed reliable model performance tracking for compound selection decisions because false positives cost $2M+ per compound, but we were constrained by limited labeled data, 6-month feedback loops, and no existing monitoring infrastructure.\nOptions considered:\nOption A: Offline validation only\nPros: Fast to implement, uses existing test sets Cons: No visibility into production drift, misses real-world degradation Risk: Models degrade silently, false confidence in predictions Option B: Online monitoring only\nPros: Real-time visibility, catches degradation early Cons: Requires production infrastructure, longer implementation time Risk: No baseline validation, unclear what \u0026ldquo;degradation\u0026rdquo; means Option C: Combined framework (offline + online)\nPros: Baseline validation + ongoing monitoring, catches both training issues and production drift Cons: More complex, requires coordination across teams Risk: Implementation complexity, potential for conflicting signals Decision: Chose Option C because:\nCompound decisions require both initial validation (offline) and ongoing confidence (online) 6-month feedback loops mean we need early warning signals, not just retrospective analysis The infrastructure investment pays off across multiple models and use cases Constraints:\nLimited labeled data for validation sets (biology team bandwidth) 3-month timeline to show value (executive commitment window) No existing monitoring infrastructure (platform team capacity) Outcome # Primary outcome: De-risked $2M+ in compound development by catching model degradation 3 months earlier than the previous process allowed. The framework identified performance drift in two models before they influenced compound selection decisions.\nGuardrails maintained:\nModel accuracy stayed within Â±2% of baseline Prediction latency remained under 100ms (required for interactive workflows) False positive rate stayed below 5% threshold Second-order effects:\nEstablished evaluation framework as standard for all discovery ML models Created reusable monitoring infrastructure used by 3 other model teams Improved stakeholder confidence in ML-driven decisions (measured via survey) Limitations acknowledged:\nEarly detection required 2-3 months of production data to establish baselines Some false alarms occurred during initial calibration (resolved through threshold tuning) Framework doesn\u0026rsquo;t prevent all bad decisions, but significantly reduces risk Reflection # What I\u0026rsquo;d do differently:\nStart with a smaller pilot model to validate the framework before scaling Involve biology team earlier in validation set design (reduced rework) Build monitoring dashboards in parallel with framework design (faster time-to-value) What this taught me about decision-making:\nEvaluation frameworks are product decisions, not just technical choices Stakeholder alignment on \u0026ldquo;what good looks like\u0026rdquo; is critical before building Combining offline and online validation creates defensible decision systems How this informs future decisions:\nAlways design evaluation before building models (not after) Establish decision forums with clear owners before problems arise Invest in reusable infrastructure when multiple teams face similar challenges ","date":"9 January 2026","externalUrl":null,"permalink":"/case-studies/preventing-metric-theater-drug-discovery-ml/","section":"Case Studies","summary":"","title":"Preventing Metric Theater in Drug Discovery ML","type":"case-studies"},{"content":"","date":"9 January 2026","externalUrl":null,"permalink":"/tags/product-strategy/","section":"Tags","summary":"","title":"Product-Strategy","type":"tags"},{"content":" Context # Late 2025 brought Montai to a strategic crossroads. Our core IP hinged on generating novel analog compounds (\u0026ldquo;Anthrologs\u0026rdquo;) through proprietary AI models â€” but the internal generative model produced ~360M virtual compounds with uncertain synthetic feasibility. Meanwhile, XtalPi offered curated, higher-confidence AI-suggested compounds from external sources.\nFacts:\nLate 2025: Montai\u0026rsquo;s core IP = generating novel analog compounds (\u0026ldquo;Anthrologs\u0026rdquo;) Problem: Internal generative model produced ~360M virtual compounds, but many not synthesizable/uncertain value Alternative: XtalPi (external partner) offered curated AI-suggested compounds (more drug-like) Stakes: Resource allocation - invest in internal model improvement OR buy external suggestions? The CSO (Margo) needed an evidence-based recommendation by December 1, 2025. This wasn\u0026rsquo;t a philosophical debate about build vs buy â€” it was a portfolio allocation decision with measurable ROI implications. I had three weeks to model the tradeoffs quantitatively and make a clear recommendation.\nOwnership # I owned:\nComparative analysis design (internal vs external compound quality) ROI modeling (compounds accessible per $ investment) Visualization strategy (UMAP plots, complexity charts for exec communication) Presentation to CSO (Dec 1 deadline) I influenced:\nStrategic direction (hybrid approach recommendation) ACN model improvement priorities (with Duminda, ML scientist) Partnership terms with XtalPi (data informed negotiations) Decision Frame # Problem statement:\nAllocate resources between internal generative model improvement and external compound partnerships to maximize discovery speed while managing IP and cost tradeoffs, constrained by:\nInternal generative model accuracy too low (needed ~50Ã— improvement) XtalPi partnership cost vs potential value unclear Time pressure (programs need compounds NOW, not in 1 year) Options considered:\nOption A: Rely internally (double down on ACN model)\nPros: IP stays in-house, potentially massive unique space Cons: Model needs significant improvement, compounds have uncertain synthetic feasibility Risk: Wasted time on low-quality suggestions, delays programs Option B: Outsource analog suggestions (XtalPi)\nPros: Immediate high-quality suggestions, less internal R\u0026amp;D Cons: Cost, reliance on external, less proprietary Risk: Dependence on partner, potential quality gaps Option C: Hybrid (external for near-term + internal for long-term)\nPros: Don\u0026rsquo;t miss opportunities today while investing in tomorrow Cons: More complex, requires patience for internal improvements Risk: Internal improvements never materialize (sunk cost) Decision: Recommended Option C because:\n[FACTS from archaeology p.14-15:]\nQuantitative analysis:\nHead-to-head comparison (STAT6, OX40L, TL1A):\nFiltered both Montai Anthrologs + XtalPi space to top quality Compared predicted activity and novelty via UMAP diversity plots XtalPi covered chemical areas Montai didn\u0026rsquo;t (complementary) ROI model (\u0026ldquo;napkin math\u0026rdquo;):\n$250k building block investment â†’ 0.9-12.4M Anthrologs accessible Scales with higher budgets, but diminishing returns without model improvement Calculated ACN precision gain needed: ~50Ã— improvement for full generative utility Strategic implication:\nExternal (XtalPi) fills gap NOW while ACN improves Set goal: If cACN v2 achieves precision by mid-2026, reduce external dependency Hybrid de-risks both options (not all-in on unproven internal model) Constraints:\nDec 1, 2025 deadline to CSO (time-boxed analysis) 360M Montai compounds vs unknown XtalPi set size (asymmetric comparison) Internal model improvement timeline uncertain (Duminda\u0026rsquo;s capacity) Outcome # Primary outcome:\nGuided strategic decision that balanced near-term progress with long-term IP:\nImmediate: Montai allocated budget to XtalPi-sourced compounds for 2-3 programs Long-term: Tasked data/ML team to focus on cACN v2/v3 improvements (precision goal) Validation: Some XtalPi compounds became hits in early 2026 (justified spend) The hybrid approach avoided overcommitting in either direction. Going all-in on internal models would have delayed programs by 6-12 months while we chased a 50Ã— accuracy improvement. Going fully external would have ceded our core IP advantage. The quantitative analysis made it clear that both paths had merit â€” and that combining them de-risked the strategy while keeping options open.\nMetrics:\nAnalysis timeframe: ~3 weeks (Nov-Dec 2025) Executive decision: Made by Dec 1, 2025 (met deadline) Outcome validation: XtalPi hits in Q1 2026 + cACN v2 improved 2Ã— by mid-2026 (on track to 50Ã—) Strategic artifacts:\nROI model spreadsheet (investment scenarios) UMAP diversity plots (XtalPi vs Montai chemical space) Complexity charts (synthetic accessibility comparison) Presentation deck to CSO (evidence-based recommendation) Second-order effects:\nTemplate for future build vs buy decisions (quantitative framework reused) Strengthened partnership with XtalPi (data-driven collaboration) Internal team focused on high-leverage improvements (not boiling ocean) Limitations acknowledged:\nAnalysis timeboxed (deeper investigation possible but not needed for decision) Model improvement uncertainty (50Ã— gain ambitious, may take longer) Hybrid complexity (managing two paths simultaneously) Reflection # What I\u0026rsquo;d do differently:\nThree strategic analysis lessons emerged in retrospect:\nStart internal model improvements earlier (not wait for crisis/comparison) Engage chemists more in generative model design (synthetic feasibility blindspot) Pilot XtalPi with one program before org-wide (reduce risk) The first was a failure of proactive planning â€” we knew the ACN model had accuracy issues but didn\u0026rsquo;t prioritize improvements until external pressure forced the comparison. The second repeated a pattern from the nomination scaling work: treating chemist expertise as validation rather than design input. The third was standard risk mitigation we should have applied but didn\u0026rsquo;t.\nWhat this taught me about decision-making:\nThis analysis validated three principles about strategic framing:\nQuantitative framing transforms opinions into evidence â€” the ROI model gave the CSO a concrete basis for decision rather than competing gut feelings about build vs buy Build vs buy is rarely binary â€” hybrid approaches often dominate pure strategies, especially when timelines and uncertainty favor hedging Executive decisions need clear options plus a recommendation â€” presenting \u0026ldquo;here\u0026rsquo;s the data, you decide\u0026rdquo; abdicates leadership; executives want your synthesis and recommendation backed by evidence How this informs future decisions:\nThese meta-takeaways now guide my approach to strategic analysis:\nAlways model tradeoffs quantitatively when stakes are high â€” even \u0026ldquo;napkin math\u0026rdquo; beats handwaving about strategic direction Build vs buy is a portfolio decision, not an all-in bet â€” maintaining optionality through hybrid approaches preserves strategic flexibility Strategic patience requires near-term wins to buy time for long bets â€” the hybrid worked because XtalPi compounds delivered wins while we improved internal models Factual Evidence Citations:\nProject Inventory p.9 (XtalPi collaboration entry) Decision Systems p.14-15 (detailed analysis narrative) Slack DM (Williamâ†’Athan analysis goal) Jake Ombach Slack (quantitative scenario results) ","date":"15 November 2025","externalUrl":null,"permalink":"/case-studies/xtalpi-build-vs-buy-analysis-montai/","section":"Case Studies","summary":"","title":"Build vs Buy: Strategic Analysis for Analog Generation","type":"case-studies"},{"content":"","date":"15 November 2025","externalUrl":null,"permalink":"/tags/build-vs-buy/","section":"Tags","summary":"","title":"Build-vs-Buy","type":"tags"},{"content":"","date":"15 November 2025","externalUrl":null,"permalink":"/tags/executive-communication/","section":"Tags","summary":"","title":"Executive-Communication","type":"tags"},{"content":"","date":"15 November 2025","externalUrl":null,"permalink":"/tags/roi-modeling/","section":"Tags","summary":"","title":"Roi-Modeling","type":"tags"},{"content":"","date":"15 November 2025","externalUrl":null,"permalink":"/tags/strategic-analysis/","section":"Tags","summary":"","title":"Strategic-Analysis","type":"tags"},{"content":"","date":"1 September 2025","externalUrl":null,"permalink":"/tags/cross-functional/","section":"Tags","summary":"","title":"Cross-Functional","type":"tags"},{"content":"","date":"1 September 2025","externalUrl":null,"permalink":"/tags/decision-frameworks/","section":"Tags","summary":"","title":"Decision-Frameworks","type":"tags"},{"content":" Context # By 2025, Montai ran multiple concurrent R\u0026amp;D experiments â€” AI model iterations, assay validations, Anthrolog generation improvements. Each experiment had implicit goals but lacked explicit success criteria. The result: debates about \u0026ldquo;when to pivot\u0026rdquo; and \u0026ldquo;when to scale\u0026rdquo; became opinion-driven rather than evidence-backed.\nFacts:\nBy 2025: Multiple concurrent experiments (AI models, assays, Anthrolog generations) Problem: Unclear success criteria per experiment (when to pivot? when to scale?) Example confusion: \u0026ldquo;AI model improved accuracy\u0026rdquo; but didn\u0026rsquo;t translate to better compound selection Stakes: Wasted months on meandering experiments without clear learning goals The core issue traced back to a fundamental principle from my PhD training: experiments without pre-defined hypotheses produce data, not learning. In academic research, you write your aims before running experiments. In biotech R\u0026amp;D, we were running experiments and retroactively deciding whether results \u0026ldquo;felt good enough.\u0026rdquo; This had to change.\nOwnership # I owned:\nFramework design (inspired by academic experimental design) Template structure (hypothesis, metrics, decision gates) Pilot with STAT6/OX40 programs Dissemination (poster, Ops presentation, team feedback integration) I influenced:\nProgram-specific agenda content (with Jake Ombach, scientists) Leadership adoption (CTO + team feedback by 10/28/25 deadline) Integration into quarterly planning (2026 OKRs) Decision Frame # Problem statement:\nEstablish a lightweight decision framework that imposes research rigor on R\u0026amp;D experiments to reduce cycle time and increase pivot clarity, constrained by:\nNo pre-existing template (creating from scratch) Risk of bureaucracy (scientists see as overhead, not value) Need exec buy-in (not just bottoms-up adoption) Options considered:\nOption A: Continue informal learning (Slack + ad-hoc meetings)\nPros: No process overhead, flexible Cons: Insights slip through cracks, repeated debates Risk: Slow iteration, missed pivots Option B: Heavyweight experimental design doc per project\nPros: Thorough, academic rigor Cons: Time-consuming, likely ignored Risk: Process theater, not actual use Option C: Lightweight \u0026ldquo;Learning Agenda\u0026rdquo; (one-page)\nPros: Quick to create, forces clarity, actionable Cons: May oversimplify complex experiments Risk: Becomes checkbox exercise if not enforced Decision: Chose Option C because:\n[FACTS from archaeology p.12-13:]\nConcise format increases adoption (one page on Confluence/poster) Pre-planned decision gates reduce debate time (agreed criteria upfront) Visible in program reviews (not hidden in docs) Example: STAT6 agenda had enrichment thresholds - stopped underperforming screen 2 weeks early Balance of rigor and pragmatism.\nConstraints:\n1-month timeline to pilot (Q3 2025 urgency) Team skepticism (scientists value science, not \u0026ldquo;frameworks\u0026rdquo;) Need exec sponsorship (CTO feedback required) Outcome # Primary outcome:\nCut decision cycle time 20% (~10 weeks â†’ ~8 weeks) while increasing stakeholder clarity on project goals:\nAdoption: All major programs (AHR, NRF2, STAT6, OX40) had agendas by late 2025 Usage: Team consulted agendas in decision meetings (not shelf-ware) Example impact: Stopped underperforming analog screen 2 weeks earlier (learning agenda guardrails triggered pivot) The cultural shift mattered more than the time savings. Learning Agendas moved the organization from opinion-driven debates (\u0026ldquo;I think this model is good enough\u0026rdquo;) to evidence-based pivots (\u0026ldquo;The agenda said we\u0026rsquo;d pivot if accuracy didn\u0026rsquo;t reach X, and it didn\u0026rsquo;t reach X\u0026rdquo;). Pre-commitment to decision criteria eliminated retrospective rationalization and made failure a legitimate outcome rather than a political liability.\nMetrics:\nDecision cycle time: ~10 weeks â†’ ~8 weeks (20% reduction, major decisions) Stakeholder clarity: 4.5/5 \u0026ldquo;understand project goals\u0026rdquo; (vs 3.8/5 before, internal survey) Adoption rate: 100% of programs in quarterly reviews (Q1 2026) Guardrails maintained:\nAgendas stayed lightweight (1 page, not doc sprawl) Flexibility preserved (could update questions if strategy changed) No blame culture (failed experiments = learning, not failure) Second-order effects:\nTemplate for other teams (engineering adopted for tech experiments) Influenced 2026 planning (every initiative needed clear success criteria) Became interview artifact (showed org maturity to candidates) Limitations acknowledged:\nUpfront time investment (kickoff slightly slower, saved time later) Some scientists initially felt constrained (\u0026ldquo;locked-in\u0026rdquo; to metrics) Framework only as good as enforcement (requires discipline) Reflection # What I\u0026rsquo;d do differently:\nThe rollout exposed gaps in my change management approach:\nPilot with friendly team first (not announce org-wide immediately) Create 2-3 example agendas before rollout (not just template) Pair with decision-making workshop (teach framework, not just distribute) The template alone wasn\u0026rsquo;t enough â€” teams needed examples and coaching to see how Learning Agendas applied to their specific experiments. By launching broadly without pilots, I created confusion and had to backfill with one-on-one sessions. A slower, example-driven rollout would have accelerated actual adoption.\nWhat this taught me about decision-making:\nThis project validated a core thesis about PhD â†’ Product skill transfer:\nAcademic experimental design translates directly to business decisions â€” the logic of hypothesis â†’ test â†’ pivot works whether you\u0026rsquo;re running gels or evaluating AI models Pre-commitment to decision criteria reduces politics â€” when stakeholders agree on thresholds before seeing data, debates shift from \u0026ldquo;is this good enough?\u0026rdquo; to \u0026ldquo;did we hit the bar?\u0026rdquo; Lightweight structure beats heavyweight docs â€” scientists adopted one-page agendas because they didn\u0026rsquo;t feel like bureaucracy; thoroughness without pragmatism kills adoption How this informs future decisions:\nThree principles now shape how I design decision systems:\nAlways define success criteria before starting work, not retroactively â€” I now refuse to approve projects without clear \u0026ldquo;we\u0026rsquo;ll pivot if X\u0026rdquo; statements Decision frameworks are products â€” they need user research, design iteration, and adoption strategies, not just documentation Cultural change requires artifacts plus enforcement â€” the Learning Agenda template worked because program reviews explicitly required agendas, not just because the doc existed Factual Evidence Citations:\nProject Inventory p.7 (Learning Agenda entry) Decision Systems p.12-13 (detailed framework description) Quantitative Outcomes (cycle time metric) Conecta Ops notes (10/28/25 feedback deadline) ","date":"1 September 2025","externalUrl":null,"permalink":"/case-studies/learning-agenda-framework-montai/","section":"Case Studies","summary":"","title":"Learning Agendas: Bringing Research Rigor to Product Decisions","type":"case-studies"},{"content":"","date":"1 September 2025","externalUrl":null,"permalink":"/tags/phd-transfer/","section":"Tags","summary":"","title":"Phd-Transfer","type":"tags"},{"content":"","date":"15 August 2025","externalUrl":null,"permalink":"/tags/data-quality/","section":"Tags","summary":"","title":"Data-Quality","type":"tags"},{"content":" Context # Mid-2025 was a period of rapid growth at Montai â€” more programs, more models, more data flowing through pipelines built for smaller scale. Technical debt had accumulated in migration work from earlier systems, creating latent risks that hadn\u0026rsquo;t yet manifested. Until STAT6.\nFacts:\nMid-2025: STAT6 program discovered predictions missing from warehouse Impact: Could not evaluate nominations for critical program ($M+ at stake) Symptom: Dashboard DR-3098 failed, analysis queries returned incomplete results Urgency: Program decisions on hold, stakeholder trust eroding The stakes extended beyond the immediate technical bug. This was organizational credibility on the line â€” scientists needed confidence that data infrastructure wouldn\u0026rsquo;t become a bottleneck to discovery. A 6-week delay on a critical program signaled deeper quality issues, and stakeholders rightfully questioned whether other datasets harbored similar problems.\nOwnership # I owned:\nIncident escalation (raised to \u0026ldquo;major severity\u0026rdquo;) Root cause analysis (traced legacy migration bug) Remediation plan design (technical + process fixes) Postmortem documentation and dissemination New governance framework (data lineage tests, release gates) I influenced:\nPlatform Engineering priorities (PE-3217 ticket) Exec communication in Conecta Ops (status updates) Cultural shift toward proactive quality (not reactive fixes) Decision Frame # Problem statement:\nResolve data integrity failure blocking STAT6 program while establishing systemic safeguards preventing recurrence, constrained by:\n258M predictions to backfill (engineering capacity) Ongoing programs needing data NOW (can\u0026rsquo;t wait for perfect fix) Root cause hidden in legacy code (archaeological debugging needed) Options considered:\nOption A: Quick patch and move on\nPros: Fast resolution, unblock STAT6 immediately Cons: No systemic improvement, likely repeats Risk: Band-aid on deeper quality issues Option B: Comprehensive rebuild\nPros: Clean slate, eliminate tech debt Cons: Months of work, programs stalled Risk: Over-engineering, new bugs introduced Option C: Targeted fix + governance uplift\nPros: Resolve immediate crisis + prevent recurrence via process Cons: Requires discipline to implement governance (not just talk) Risk: Process additions seen as bureaucracy Decision: Chose Option C because:\n[FACTS from archaeology p.28-29:]\nImmediate: Backfill missing predictions (PE-3217 Jira ticket) Short-term: Add automated lineage checks (task_id = task_name validation) Long-term: Establish data ownership and release gates Cultural: Postmortem as learning artifact (not blame doc) This balanced urgency with sustainability.\nConstraints:\n6-week timeline already impacting program (pressure for quick fix) Small Platform Eng team (Rody\u0026rsquo;s capacity) Need stakeholder confidence restoration (not just technical solution) Outcome # Primary outcome:\nResolved STAT6 blocker within 6 weeks AND established data quality culture preventing recurrence:\nImmediate: 258M predictions backfilled, STAT6 nominations resumed Systemic: Zero major data incidents post-fix (Q4 2025 - Q1 2026) Cultural: Postmortem process adopted (3-page report became template) The real win wasn\u0026rsquo;t fixing one bug â€” it was transforming a crisis into a capability upgrade. The postmortem created organizational memory, automated lineage checks prevented similar failures, and ownership assignments clarified accountability. What could have been a \u0026ldquo;fire-fighting\u0026rdquo; moment became the foundation for proactive data quality governance.\nMetrics:\nCrisis duration: 6 weeks to full resolution Data incidents: 3 (2024) â†’ 1 (2025) â†’ 0 (early 2026) Governance artifacts: Postmortem doc, automated lineage tests, owner assignments Stakeholder confidence: Rebuilt via transparency (Slack, Ops updates) Guardrails maintained:\nPrograms never received bad data (halted vs wrong data) Trust recovered via candid communication (not spin) No blame culture (postmortem focused on systems, not people) Second-order effects:\nIncident response playbook for future crises \u0026ldquo;Data health\u0026rdquo; monitoring dashboard (proactive vs reactive) Mentored junior engineer into \u0026ldquo;Data QA\u0026rdquo; owner role Template for other teams\u0026rsquo; postmortems (eng, product) Limitations acknowledged:\n6-week delay was painful (could\u0026rsquo;ve been faster if monitoring existed) Some false alarms during post-fix calibration (resolved via threshold tuning) Process additions require ongoing discipline (risk of decay) Reflection # What I\u0026rsquo;d do differently:\nThe retrospective is painful but instructive:\nHeeded early Slack warnings (Sep-Oct 2024) about ID confusion Built monitoring infrastructure proactively (not reactively post-crisis) Escalated to \u0026ldquo;major severity\u0026rdquo; immediately (not after days of debugging) The first two represent failures of prioritization â€” I saw the signals but didn\u0026rsquo;t allocate capacity to de-risk before problems manifested. The third was a judgment error during the incident itself: initial debugging felt routine until it wasn\u0026rsquo;t, and we lost days before mobilizing full resources.\nWhat this taught me about decision-making:\nThree leadership lessons emerged:\nCrises reveal latent process gaps â€” the incident exposed governance weaknesses that had existed for months; treating it as a learning opportunity rather than blame exercise turned pain into progress Transparency beats spin when rebuilding stakeholder trust â€” candid Slack updates and Ops presentations acknowledged the delay and outlined specific fixes, which restored confidence faster than downplaying severity Postmortems work when blameless and action-focused â€” the 3-page report focused on systems and processes, not individual mistakes, making it a template other teams adopted How this informs future decisions:\nThese principles now guide my approach to operational risk:\nInvest in monitoring before crises hit â€” proactive instrumentation costs less than reactive debugging, and I now allocate capacity to observability upfront Small data quirks deserve root-causing â€” anomalies are signals of deeper issues; don\u0026rsquo;t let them linger Cultural change requires artifacts â€” the postmortem template, automated tests, and ownership assignments institutionalized the learning; without those artifacts, the lessons would have faded Factual Evidence Citations:\nIncident Postmortem: ACN Lineage/STAT6 (Confluence doc) Quantitative Outcomes p.33 (incidents metric) Failures \u0026amp; Pivots p.28-29 (detailed narrative) Conecta Ops notes (escalation timeline) ","date":"15 August 2025","externalUrl":null,"permalink":"/case-studies/stat6-data-crisis-response-montai/","section":"Case Studies","summary":"","title":"From Data Crisis to Data Culture: The STAT6 Incident","type":"case-studies"},{"content":"","date":"15 August 2025","externalUrl":null,"permalink":"/tags/incident-response/","section":"Tags","summary":"","title":"Incident-Response","type":"tags"},{"content":"","date":"15 August 2025","externalUrl":null,"permalink":"/tags/postmortem/","section":"Tags","summary":"","title":"Postmortem","type":"tags"},{"content":"","date":"15 August 2025","externalUrl":null,"permalink":"/tags/team-leadership/","section":"Tags","summary":"","title":"Team-Leadership","type":"tags"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/developer-experience/","section":"Tags","summary":"","title":"Developer-Experience","type":"tags"},{"content":" Context # By mid-2024, Montai\u0026rsquo;s internal web app landscape had fragmented. Different engineers built tools in their preferred frameworks â€” Python Streamlit, Python Dash, R Shiny, Jupyter notebooks â€” creating a sprawling ecosystem with inconsistent UX and duplicated effort. For a small data team (~5-6 people), this fragmentation imposed hidden costs: context-switching overhead, maintenance burden, and harder onboarding.\nFacts:\n2024: Growing need for internal web apps (compound selection, data visualization, report generation) Problem: Different engineers using different frameworks (Python Streamlit/Dash, R Shiny, Jupyter notebooks) Pain: Inconsistent UX, duplicated effort, hard to maintain, context-switching cost Stakes: Small team (~5-6 people) needed velocity + consistency Technical leadership was needed to converge on a single approach. The challenge: balance team skills, deployment infrastructure, and use case requirements â€” while avoiding the trap of \u0026ldquo;one size fits all\u0026rdquo; dogma that ignores practical constraints.\nOwnership # I owned:\nFramework evaluation (Streamlit vs Dash vs Shiny vs multi-framework) Proof-of-concept implementation (Nomination App in Shiny) Internal standards documentation (app dev guidelines) Team training (brown bag sessions on Shiny patterns) I influenced:\nMontai Style library (with Kyle T - UI components for Shiny) Posit Connect deployment strategy (with Platform Eng) Team skill development (mentored colleagues on Shiny) Decision Frame # Problem statement:\nChoose and standardize a single internal app framework to accelerate development velocity and establish consistent UX, constrained by:\nMixed team skills (some Python, some R, few full-stack engineers) Data-heavy use cases (need robust plotting + data wrangling) Small team (can\u0026rsquo;t support multiple frameworks well) Options considered:\nOption A: Streamlit (Python)\nPros: Quick to develop, popular, Python-based (most engineers know) Cons: Lacked flexibility for complex UI at the time, less mature Risk: Montai had significant R analytics codebase to leverage Option B: Dash (Python/Plotly)\nPros: More customizable than Streamlit, production-ready Cons: More engineering-heavy, steeper learning curve Risk: Fewer team members comfortable with Plotly ecosystem Option C: R Shiny\nPros: Powerful for data apps, integrates with R analytics, Posit Connect deployment, data team knows R Cons: Fewer pure software engineers comfortable with R Risk: Long-term maintainability if team shifts to Python-heavy Option D: Multi-framework (case-by-case)\nPros: Flexibility, use best tool per job Cons: Fragmentation persists, no consistency gains Risk: Maintenance burden grows, onboarding harder Decision: Chose Option C (R Shiny) because:\n[FACTS from archaeology p.17-18:]\nTeam skills alignment: Data scientists already used R for analysis (reuse code directly in apps) Deployment simplicity: Posit Connect already in use (easy publishing) Rapid prototyping: Shiny allows quick UI iteration with reactive data Ecosystem maturity: Robust R packages (ggplot2, dplyr) for data visualization Proof-of-concept success: Built Nomination App as flagship example (validated approach) Tradeoff accepted: Some engineers less comfortable with R, but data team velocity more valuable.\nConstraints:\n3-month timeline to show value (mid-2024 goal) No dedicated UI/UX designer (needed framework with decent defaults) Small Platform Eng capacity (deployment needed to be straightforward) Outcome # Primary outcome:\nCut app development time ~66% (3 weeks â†’ 1 week for new apps) while establishing consistent UI/UX:\nVelocity: SAR Dashboard app built in 1 week using Shiny template (vs 3 weeks from scratch) Consistency: All internal apps shared Montai Style components (auth, layout, branding) Capability: Non-engineer (chemist) added filter to Shiny app after training (broadened contributor base) Standardization enabled a small team to punch above its weight. By converging on R Shiny, we transformed app development from bespoke engineering work into repeatable application of templates and patterns. The Montai Style library meant new apps inherited authentication, theming, and layout automatically â€” developers focused on domain logic, not infrastructure.\nMetrics:\nApp development time: 3 weeks â†’ 1 week (anecdotal, for similar scope) Framework adoption: 100% of new apps in Shiny by Q4 2024 Team contributions: Chemist contributed feature (skill development success) Technical artifacts:\nNomination App (flagship proof-of-concept) Montai Style repo (UI components library for Shiny) Internal dev guidelines (Confluence: \u0026ldquo;How to build a Shiny app\u0026rdquo;) Second-order effects:\nTemplate for other internal tools (SAR Dashboard, compound selection) Reusable patterns (authentication, data connections, theming) Easier onboarding (new hires learned one framework, not three) Limitations acknowledged:\nNomination App \u0026ldquo;still in early stages\u0026rdquo; by mid-2024 (incomplete proof-of-concept initially) Some engineers less comfortable with R (required training investment) Framework lock-in risk (if future needs exceed Shiny capabilities) Reflection # What I\u0026rsquo;d do differently:\nThree technical decision lessons stand out:\nBuild 2-3 small apps as proof-of-concept (not one large incomplete app) Pair with full-stack engineer earlier (balanced R expertise with UI polish) Evaluate hybrid (Shiny prototype â†’ React for scale) for future-proofing The Nomination App as flagship proof-of-concept was the right instinct but wrong execution â€” it remained \u0026ldquo;in early stages\u0026rdquo; too long, undermining confidence in the standardization decision. Building multiple small, complete examples would have demonstrated feasibility more convincingly. The other two are standard risk mitigations I should have applied upfront.\nWhat this taught me about decision-making:\nFramework standardization reinforced three architecture principles:\nStandardization is both social and technical â€” adoption matters more than theoretical \u0026ldquo;best tool\u0026rdquo;; R Shiny won because the data team already used R for analysis, not because it was objectively superior to Python frameworks Proof-of-concept validates assumptions better than research â€” actual implementation exposed deployment constraints and UI limitations that documentation never revealed Small team constraints favor simplicity over flexibility â€” supporting one framework well beats supporting many poorly; the tradeoff was clear and correct How this informs future decisions:\nThese tool selection principles now guide my technical architecture work:\nAlways prototype with a real use case before mandating a standard â€” abstract evaluation misses practical constraints that only implementation reveals Optimize for team strengths, not abstract \u0026ldquo;best practices\u0026rdquo; â€” the best framework is the one your team can actually use effectively Standardization trades flexibility for velocity â€” know when that tradeoff is worth it (small teams yes, large platform teams maybe not) Factual Evidence Citations:\nProject Inventory p.8 (Shiny framework entry) Technical Architecture p.17-18 (decision rationale) 2024 Mid-Year Review (Shiny standardization goal) Slack: Montai Style repo ready (Kyle T) ","date":"1 June 2024","externalUrl":null,"permalink":"/case-studies/shiny-framework-standardization-montai/","section":"Case Studies","summary":"","title":"Standardizing Montai's App Ecosystem with R Shiny","type":"case-studies"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/team-velocity/","section":"Tags","summary":"","title":"Team-Velocity","type":"tags"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/tool-selection/","section":"Tags","summary":"","title":"Tool-Selection","type":"tags"},{"content":" Context # Early 2023 presented a defining challenge: Montai\u0026rsquo;s AI models could predict activity across millions of compounds, but manual library creation processes were bottlenecked at ~250 compounds per program. The central question wasn\u0026rsquo;t whether AI could generate predictions â€” it was whether we could build a scalable system that maintained scientific rigor while expanding the search space 20Ã—.\nFacts:\nBaseline: 100â€™s of compounds, chosen manually for screening from within existing library Stakes: Scale 10Ã— to 100Ã— per program to enabled by bioactivity ML models Environment: Early-stage biotech, unproven concept My role: First data science/product hire, architected pipeline The challenge # How do you architect a multi-objective decision system, that provides an optimal starting point for drug discovery funnels, is understandable by all the key decision-makers at the organization?\nThis was a multi-faceted problem â€” ML scientists wanted maximum chemical diversity, medicinal chemists needed synthetic feasibility, and program leads to ensure they didnâ€™t waste their teamâ€™s time on a batch of inactive compounds. Each stakeholder brought valid constraints â€” the pipeline needed to serve all groups while delivering high quality recommendations that would bolster confidence in the AI approach.\nThe solution # I owned:\nEnd-to-end â€˜nominationâ€™ pipeline architecture: Our team constructed an orchestrated SQL pipeline that enabled fully traceable, human-interpretable ML integration strategy (model outputs â†’ analytics) Data quality framework for predictions Phased rollout strategy (MVP â†’ scale â†’ quality) I influenced:\nModel selection criteria (with Duminda Ranasinghe, Lead ML Scientist) Nomination criteria per program (with Jake Ombach, Computational Biologist) External data partnerships (XtalPi, vendor libraries) Decision Frame # Problem statement:\nBuild a data pipeline that scales compound nominations 20Ã— while maintaining or improving experimental hit rates, constrained by:\nUnproven AI prediction quality Limited ML engineering capacity (small team) Need for rapid learning cycles (not perfect first try) Options considered:\nOption A: Conservative scale (500-1000 nominations)\nPros: Lower risk, manageable if quality issues emerge Cons: May miss opportunities in vast chemical space Risk: Underutilize AI capability, slow learning Option B: Aggressive scale (10,000+ immediately)\nPros: Maximum coverage of chemical space Cons: Drowning in low-quality candidates, scientist overload Risk: Lost trust if hit rate crashes, wasted experimental capacity Option C: Phased scaling with quality gates\nPros: Learn at each phase, adjust criteria, build confidence Cons: More complex coordination, requires patience Risk: Slower initial progress Decision: Chose Option C because:\n[FACTS from archaeology p.20-21:]\nPhase 1 (2023): Baseline nomination (prove concept with known compounds) Phase 2 (2024): Expand to 1000+ (maximize learning via generative + commercial) Phase 3 (late 2024): Quality filters (diversity, confidence thresholds) Sequencing logic: Prove â†’ scale â†’ refine (not perfect upfront)\nConstraints:\nPipeline latency: Manual processes took 2-3 days â†’ needed automation Data volume: 10M compounds (2023) â†’ 258M predictions (2024) Team capacity: Solo data lead initially, growing to 5-6 by 2024 Outcome # Primary outcome:\nScaled from 250 â†’ 6,500+ nominations per program (26Ã— increase) while IMPROVING hit-to-lead rates:\nTNFR1: 27% hit-to-lead (vs ~5% baseline) Multiple programs advanced to lead optimization faster The significance: This wasn\u0026rsquo;t just volume scaling â€” quality improved alongside throughput. By implementing phased quality gates and leveraging diverse data sources (generative models, commercial libraries, XtalPi partnerships), we validated that AI-driven discovery could outperform manual selection. This became Montai\u0026rsquo;s core operational advantage and a key narrative for fundraising.\nMetrics:\nNomination throughput: 250 â†’ 5,000-7,000 per program (Q4 2024) Hit-to-lead conversion: 5% â†’ 15-30% range Pipeline latency: 2-3 days â†’ same-day updates Model predictions: 10M â†’ 258M compounds scored Guardrails maintained:\nNomination quality didn\u0026rsquo;t degrade with scale (improved filters offset volume) Scientist time per compound review stayed manageable (self-service dashboards) Data infrastructure handled 10Ã— load increase without major incidents (until STAT6 - separate case) Second-order effects:\nEnabled XtalPi partnership analysis (build vs buy decision) Created reusable pipeline for future programs (MARS, cACN v2/v3) Demonstrated industrialized discovery to investors (fundraising narrative) Limitations acknowledged:\nDiminishing returns emerged at ~5K nominations (quality \u0026gt; quantity phase needed) Generated Anthrologs initially unusable (separate failure/pivot story) Pipeline automation never fully complete (some manual triggers remained) Reflection # What I\u0026rsquo;d do differently:\nLooking back at the archaeology of this work, three decisions stand out as suboptimal:\nStart with tighter nomination criteria earlier (wasted effort on low-probability compounds in Phase 2) Invest in monitoring infrastructure upfront (reactive vs proactive on data quality) Engage chemists more in generative model development (synthetic feasibility blindspot) The first two were classic \u0026ldquo;move fast and learn\u0026rdquo; tradeoffs that proved correct in hindsight â€” we needed the volume data to understand quality needs. The third was a genuine miss: treating synthetic feasibility as a post-generation filter rather than baking it into model training cost us months.\nWhat this taught me about decision-making:\nThree principles emerged that I\u0026rsquo;ve since applied consistently:\nPhased rollouts with learning gates beat perfect upfront design â€” you can\u0026rsquo;t architect your way out of uncertainty, you build to learn Volume metrics mislead without quality tracking â€” nomination count was a vanity metric until we paired it with hit-to-lead conversion Stakeholder confidence requires visible iteration â€” scientists trusted the pipeline because they saw us adjust criteria based on their feedback, not because the first version was perfect How this informs future decisions:\nThese lessons directly shaped my approach to subsequent projects:\nAlways define success criteria per phase before executing â€” the Learning Agenda framework codified this Build quality frameworks alongside feature development, not after crises â€” the STAT6 incident reinforced this Balance exploration (maximize learning) with exploitation (optimize known strategies) â€” this framing now guides my portfolio thinking Factual Evidence Citations:\nDrug Project Overviews.xlsx (nomination counts per program) 2024 Mid-Year Review (pipeline automation goals) Quantitative Outcomes Inventory p.30 (metrics table) Product Strategy case study p.20-21 (phasing logic) ","date":"1 June 2023","externalUrl":null,"permalink":"/case-studies/scaling-ai-nominations-montai/","section":"Case Studies","summary":"","title":"Scaling AI-Driven Drug Nominations from 250 to 7,000 Compounds","type":"case-studies"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]